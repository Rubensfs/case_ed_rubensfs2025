{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7b91d278-3fd5-41c8-906e-59c1da601884",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Databricks notebook: Bronze - Risco de Fogo(Di√°rio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d5c52bff-012f-404b-ad99-aaba05f9bca8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ================================================================\n",
    "# Bronze - Importar arquivo NetCDF (INPE Fire Risk)\n",
    "# ================================================================\n",
    "from pyspark.sql import SparkSession, functions as F\n",
    "import os\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Bronze_FireRisk_NetCDF\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5667c363-0174-4385-9418-4c3a048533c9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ================================================================\n",
    "# Par√¢metro recebido via Job Databricks\n",
    "# ================================================================\n",
    "dbutils.widgets.text(\"data_ref_carga\", \"\")\n",
    "data_ref_carga = dbutils.widgets.get(\"data_ref_carga\")\n",
    "\n",
    "if not data_ref_carga:\n",
    "    raise ValueError(\"‚ùå Par√¢metro 'data_ref_carga' n√£o informado (formato esperado: yyyy-MM-dd)\")\n",
    "\n",
    "print(f\"üóìÔ∏è Data de refer√™ncia da carga: {data_ref_carga}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "868df854-ca95-4931-b7b7-ac7902664e9f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ================================================================\n",
    "# Configura√ß√µes\n",
    "# ================================================================\n",
    "catalog = \"amazonia_catalog\"\n",
    "schema = \"b_inep\"\n",
    "table_name = \"d_fire_risk\"\n",
    "\n",
    "path_raw = \"/Volumes/amazonia_catalog/raw/raw_inpe\"\n",
    "file_name = f\"INPE_FireRiskModel_2.2_FireRisk_{data_ref_carga.replace('-', '')}.nc\"\n",
    "file_path = f\"{path_raw}/{file_name}\"\n",
    "\n",
    "print(f\"üìÇ Procurando arquivo: {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "062b39ed-99af-453d-b894-7c8694a09b5a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#spark.read.format(\"binaryFile\").option(\"pathGlobFilter\", \"*.nc\").load(\"file:/Volumes/amazonia_catalog/raw/raw_inpe\").inputFiles()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f26186e3-c9ba-4825-8cdc-0a707c377727",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# ================================================================\n",
    "# 1Ô∏è‚É£ Ler arquivo bin√°rio (.nc)\n",
    "# ================================================================\n",
    "# O formato 'binaryFile' l√™ qualquer arquivo bin√°rio, gerando colunas: path, modificationTime, length, content\n",
    "try:\n",
    "    df_bin = (\n",
    "        spark.read.format(\"binaryFile\")\n",
    "        .load(file_path)\n",
    "        .withColumn(\"data_ref_carga\", F.lit(data_ref_carga))\n",
    "        .withColumn(\"nome_arquivo\", F.lit(file_name))\n",
    "    )\n",
    "except Exception as e:\n",
    "    raise FileNotFoundError(f\"‚ùå Erro ao ler o arquivo bin√°rio: {e}\")\n",
    "\n",
    "if df_bin.count() == 0:\n",
    "    raise FileNotFoundError(f\"‚ö†Ô∏è Nenhum arquivo encontrado no caminho informado: {file_path}\")\n",
    "else:\n",
    "    print(f\"‚úÖ Arquivo lido com sucesso ({df_bin.count()} registro).\")\n",
    "\n",
    "df_bin.display(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3b4e768f-cb45-4147-b7a6-5f2cc18e41cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    df_bin = (\n",
    "        spark.read.format(\"binaryFile\")\n",
    "        # O option(\"pathGlobFilter\", \"*.nc\") √© redundante, mas inofensivo\n",
    "        .load(file_path)\n",
    "        # O .inputFiles() estava sintaticamente incorreto aqui, foi removido.\n",
    "        .withColumn(\"data_ref_carga\", F.lit(data_ref_carga))\n",
    "        .withColumn(\"nome_arquivo\", F.lit(file_name))\n",
    "    )\n",
    "except Exception as e:\n",
    "    # Capturando a exce√ß√£o de forma mais espec√≠fica para FileNotFoundError,\n",
    "    # caso o caminho esteja certo, mas o arquivo n√£o exista.\n",
    "    if \"No such file or directory\" in str(e):\n",
    "        raise FileNotFoundError(f\"‚ùå Arquivo n√£o encontrado no caminho: {file_path}\")\n",
    "    else:\n",
    "        # Relan√ßa se for outro erro\n",
    "        raise Exception(f\"‚ùå Erro inesperado ao ler o arquivo: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a658ed72-82d3-4bd0-81cc-55d6d18f92b9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, BinaryType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7c20af52-a46d-489d-adee-4f46b45ebd81",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ================================================================\n",
    "# 2Ô∏è‚É£ Quebra de Linhagem MANUAL (CR√çTICO)\n",
    "# ================================================================\n",
    "\n",
    "print(\"üõ†Ô∏è Quebrando a linhagem do arquivo de origem...\")\n",
    "\n",
    "# 1. Coletar a √∫nica linha do DataFrame\n",
    "# Isso √© seguro porque df_bin tem apenas 1 linha. A falha no .toPandas()\n",
    "# sugere um problema na *execu√ß√£o do plano de leitura*, mas a coleta simples\n",
    "# √© o √∫nico caminho restante. Se falhar aqui, o arquivo n√£o est√° acess√≠vel.\n",
    "try:\n",
    "    row = df_bin.collect()[0]\n",
    "except Exception as e:\n",
    "    # Se a coleta falhar, o PATH_NOT_FOUND √© o erro de leitura, e n√£o de grava√ß√£o.\n",
    "    raise Exception(f\"‚ùå Erro final na coleta da linha. O arquivo pode n√£o estar acess√≠vel ao cluster no caminho: {file_path}. Detalhe: {e}\")\n",
    "\n",
    "\n",
    "# 2. Extrair o conte√∫do e metadados\n",
    "content_bytes = row[\"content\"]\n",
    "data_ref_carga_val = row[\"data_ref_carga\"]\n",
    "nome_arquivo_val = row[\"nome_arquivo\"]\n",
    "\n",
    "# 3. Definir o esquema final (sem as colunas problem√°ticas: path, length, modTime)\n",
    "final_schema = StructType([\n",
    "    StructField(\"content\", BinaryType(), True),\n",
    "    StructField(\"data_ref_carga\", StringType(), True),\n",
    "    StructField(\"nome_arquivo\", StringType(), True)\n",
    "])\n",
    "\n",
    "# 4. Criar um novo DataFrame PySpark do ZERO, sem linhagem\n",
    "df_para_gravar = spark.createDataFrame(\n",
    "    [\n",
    "        (content_bytes, data_ref_carga_val, nome_arquivo_val)\n",
    "    ],\n",
    "    schema=final_schema\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b5250f7f-258e-4315-94b5-aad1b1ee12f0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ================================================================\n",
    "# 2Ô∏è‚É£ Gravar em tabela Delta particionada\n",
    "# ================================================================\n",
    "# 3. Executar a Grava√ß√£o Delta\n",
    "(\n",
    "    df_para_gravar.write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"append\")\n",
    "    .partitionBy(\"data_ref_carga\")\n",
    "    .saveAsTable(full_table_name)\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Tabela Bronze criada/atualizada: {full_table_name}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "ingesta_d_risco_fogo",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
