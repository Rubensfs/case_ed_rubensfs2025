{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a14e4930-8a52-4298-9ea3-d5d442c99b5b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# =============================================================\n",
    "# STREAMING INPE FOCOS (10min) — Databricks + Delta Live\n",
    "# =============================================================\n",
    "# Autor: Rubens F.\n",
    "# Atualização: 2025-11-12\n",
    "# Melhoria:\n",
    "# - Baixa apenas os arquivos referentes ao dia informado (AAAAMMDD)\n",
    "# =============================================================\n",
    "\n",
    "import os, re, time, requests, threading\n",
    "from datetime import datetime\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import input_file_name\n",
    "\n",
    "# ==============================\n",
    "# CONFIGURAÇÕES DE DATA E URL\n",
    "# ==============================\n",
    "DATA_ALVO = \"20251112\"  # <-- altere aqui o dia de interesse (AAAAMMDD)\n",
    "BASE_URL = \"https://dataserver-coids.inpe.br/queimadas/queimadas/focos/csv/10min/\"\n",
    "HEADERS = {\"User-Agent\": \"inpe-focos-stream/1.3\"}\n",
    "INTERVALO = 600  # segundos entre checagens (10 min)\n",
    "MIN_TAMANHO = 1024  # bytes mínimos para considerar CSV válido\n",
    "\n",
    "# ==============================\n",
    "# PATHS DO VOLUME DATABRICKS\n",
    "# ==============================\n",
    "VOLUME_BASE = \"/Volumes/datamasters/raw/raw_inpe/stream\"\n",
    "LANDING_PATH = f\"{VOLUME_BASE}/inpe_in/{DATA_ALVO}\"  # separa por dia\n",
    "CHECKPOINT_PATH = f\"{VOLUME_BASE}/_checkpoints/inpe_focos/{DATA_ALVO}\"\n",
    "SCHEMA_PATH = f\"{VOLUME_BASE}/_schemas/inpe_focos/{DATA_ALVO}\"\n",
    "DESTINO_DELTA = f\"/Volumes/datamasters/raw/raw_inpe/{DATA_ALVO}\"\n",
    "\n",
    "# Cria pastas se não existirem\n",
    "for path in [LANDING_PATH, CHECKPOINT_PATH, SCHEMA_PATH]:\n",
    "    dbutils.fs.mkdirs(f\"dbfs:{path}\")\n",
    "\n",
    "# ==============================\n",
    "# SPARK SESSION\n",
    "# ==============================\n",
    "spark = SparkSession.builder.appName(f\"INPE-Focos-Stream-{DATA_ALVO}\").getOrCreate()\n",
    "\n",
    "# ==============================\n",
    "# FUNÇÃO: LISTAR CSVs DO SITE\n",
    "# ==============================\n",
    "def listar_csvs_remotos():\n",
    "    r = requests.get(BASE_URL, headers=HEADERS, timeout=30)\n",
    "    r.raise_for_status()\n",
    "    arquivos = re.findall(r'\"([^\"]+\\.csv)\"', r.text)\n",
    "    # Filtra somente os que contenham a data desejada\n",
    "    return [a for a in arquivos if DATA_ALVO in a]\n",
    "\n",
    "# ==============================\n",
    "# FUNÇÃO: BAIXAR NOVOS ARQUIVOS\n",
    "# ==============================\n",
    "def baixar_csvs_continuamente():\n",
    "    baixados = set()\n",
    "    print(f\"Iniciando captura do INPE ({DATA_ALVO}) a cada {INTERVALO}s...\")\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            for nome in listar_csvs_remotos():\n",
    "                if nome not in baixados:\n",
    "                    url = BASE_URL + nome\n",
    "                    r = requests.get(url, headers=HEADERS, timeout=60)\n",
    "                    if r.status_code == 200 and len(r.content) > MIN_TAMANHO:\n",
    "                        destino = f\"dbfs:{LANDING_PATH}/{nome}\"\n",
    "                        tmpfile = f\"/tmp/{nome}\"\n",
    "                        with open(tmpfile, \"wb\") as f:\n",
    "                            f.write(r.content)\n",
    "                        dbutils.fs.cp(f\"file:{tmpfile}\", destino)\n",
    "                        os.remove(tmpfile)\n",
    "                        print(f\"[✔] Baixado {nome}\")\n",
    "                        baixados.add(nome)\n",
    "                    else:\n",
    "                        print(f\"[✖] Falha {nome}: tamanho_invalido\")\n",
    "            time.sleep(INTERVALO)\n",
    "        except Exception as e:\n",
    "            print(f\"[ERRO] {e}\")\n",
    "            time.sleep(120)\n",
    "\n",
    "# ==============================\n",
    "# FUNÇÃO: INICIAR STREAM\n",
    "# ==============================\n",
    "def iniciar_stream():\n",
    "    print(f\"Iniciando stream: {LANDING_PATH} → {DESTINO_DELTA}\")\n",
    "    df_stream = (\n",
    "        spark.readStream\n",
    "        .format(\"cloudFiles\")\n",
    "        .option(\"cloudFiles.format\", \"csv\")\n",
    "        .option(\"cloudFiles.schemaLocation\", SCHEMA_PATH)\n",
    "        .option(\"header\", True)\n",
    "        .load(LANDING_PATH)\n",
    "        .withColumn(\"arquivo_origem\", input_file_name())\n",
    "    )\n",
    "\n",
    "    (\n",
    "        df_stream.writeStream\n",
    "        .format(\"delta\")\n",
    "        .option(\"checkpointLocation\", CHECKPOINT_PATH)\n",
    "        .outputMode(\"append\")\n",
    "        .start(DESTINO_DELTA)\n",
    "    )\n",
    "\n",
    "# ==============================\n",
    "# EXECUÇÃO PRINCIPAL\n",
    "# ==============================\n",
    "if __name__ == \"__main__\":\n",
    "    t = threading.Thread(target=baixar_csvs_continuamente, daemon=True)\n",
    "    t.start()\n",
    "    iniciar_stream()\n",
    "    while True:\n",
    "        time.sleep(300)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Inpe_Focos_queim_Stream_10",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
